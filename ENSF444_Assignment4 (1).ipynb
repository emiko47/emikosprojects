{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {
    "id": "92778525"
   },
   "source": [
    "<font size=\"+3\"><b>Assignment 4: Pipelines and Hyperparameter Tuning</b></font>\n",
    "\n",
    "***\n",
    "* **Full Name** = Emiko Emiko\n",
    "* **UCID** = 30161505\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {
    "id": "ce31b39a"
   },
   "source": [
    "<font color='Blue'>\n",
    "In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models, and evaluate the results. More details for each step can be found below. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T0uItvnoRoUB",
   "metadata": {
    "id": "T0uItvnoRoUB"
   },
   "source": [
    "<font color='Red'>\n",
    "For this assignment, in addition to your .ipynb file, please also attach a PDF file. To generate this PDF file, you can use the print function (located under the \"File\" within Jupyter Notebook). Name this file ENGG444_Assignment##__yourUCID.pdf (this name is similar to your main .ipynb file). We will evaluate your assignment based on the two files and you need to provide both.\n",
    "</font>\n",
    "\n",
    "\n",
    "|         **Question**         | **Point(s)** |\n",
    "|:----------------------------:|:------------:|\n",
    "|  **1. Preprocessing Tasks**  |              |\n",
    "|              1.1             |       2      |\n",
    "|              1.2             |       2      |\n",
    "|              1.3             |       4      |\n",
    "| **2. Pipeline and Modeling** |              |\n",
    "|              2.1             |       3      |\n",
    "|              2.2             |       6      |\n",
    "|              2.3             |       5      |\n",
    "|              2.4             |       3      |\n",
    "|     **3. Bonus Question**    |     **2**    |\n",
    "|           **Total**          |    **25**    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OpeMjIV9VLgM",
   "metadata": {
    "id": "OpeMjIV9VLgM"
   },
   "source": [
    "## **0. Dataset**\n",
    "\n",
    "This data is a subset of the **Heart Disease Dataset**, which contains information about patients with possible coronary artery disease. The data has **14 attributes** and **294 instances**. The attributes include demographic, clinical, and laboratory features, such as age, sex, chest pain type, blood pressure, cholesterol, and electrocardiogram results. The last attribute is the **diagnosis of heart disease**, which is a categorical variable with values from 0 (no presence) to 4 (high presence). The data can be used for **classification** tasks, such as predicting the presence or absence of heart disease based on the other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "YiaUdCQYVWj-",
   "metadata": {
    "id": "YiaUdCQYVWj-",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>120.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>140.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>170.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>130.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>155.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>180.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>130.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>294 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0     28    1   2     130.0  132.0  0.0      2.0    185.0    0.0      0.0   \n",
       "1     29    1   2     120.0  243.0  0.0      0.0    160.0    0.0      0.0   \n",
       "2     29    1   2     140.0    NaN  0.0      0.0    170.0    0.0      0.0   \n",
       "3     30    0   1     170.0  237.0  0.0      1.0    170.0    0.0      0.0   \n",
       "4     31    0   2     100.0  219.0  0.0      1.0    150.0    0.0      0.0   \n",
       "..   ...  ...  ..       ...    ...  ...      ...      ...    ...      ...   \n",
       "289   52    1   4     160.0  331.0  0.0      0.0     94.0    1.0      2.5   \n",
       "290   54    0   3     130.0  294.0  0.0      1.0    100.0    1.0      0.0   \n",
       "291   56    1   4     155.0  342.0  1.0      0.0    150.0    1.0      3.0   \n",
       "292   58    0   2     180.0  393.0  0.0      0.0    110.0    1.0      1.0   \n",
       "293   65    1   4     130.0  275.0  0.0      1.0    115.0    1.0      1.0   \n",
       "\n",
       "     slope  ca  thal  num  \n",
       "0      NaN NaN   NaN    0  \n",
       "1      NaN NaN   NaN    0  \n",
       "2      NaN NaN   NaN    0  \n",
       "3      NaN NaN   6.0    0  \n",
       "4      NaN NaN   NaN    0  \n",
       "..     ...  ..   ...  ...  \n",
       "289    NaN NaN   NaN    1  \n",
       "290    2.0 NaN   NaN    1  \n",
       "291    2.0 NaN   NaN    1  \n",
       "292    2.0 NaN   7.0    1  \n",
       "293    2.0 NaN   NaN    1  \n",
       "\n",
       "[294 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data source link\n",
    "_link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data'\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame, considering '?' as missing values\n",
    "df = pd.read_csv(_link, na_values='?',\n",
    "                 names=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',\n",
    "                        'restecg', 'thalach', 'exang', 'oldpeak', 'slope',\n",
    "                        'ca', 'thal', 'num'])\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlcrJpGLWBOH",
   "metadata": {
    "id": "mlcrJpGLWBOH"
   },
   "source": [
    "# **1. Preprocessing Tasks**\n",
    "\n",
    "- **1.1** Find out which columns have more than 60% of their values missing and drop them from the data frame. Explain why this is a reasonable way to handle these columns. **(2 Points)**\n",
    "\n",
    "- **1.2** For the remaining columns that have some missing values, choose an appropriate imputation method to fill them in. You can use the `SimpleImputer` class from `sklearn.impute` or any other method you prefer. Explain why you chose this method and how it affects the data. **(2 Points)**\n",
    "\n",
    "- **1.3** Assign the `num` column to the variable `y` and the rest of the columns to the variable `X`. The `num` column indicates the presence or absence of heart disease based on the angiographic disease status of the patients. Create a `ColumnTransformer` object that applies different preprocessing steps to different subsets of features. Use `StandardScaler` for the numerical features, `OneHotEncoder` for the categorical features, and `passthrough` for the binary features. List the names of the features that belong to each group and explain why they need different transformations. You will use this `ColumnTransformer` in a pipeline in the next question. **(4 Points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yyRJQ25hXHNF",
   "metadata": {
    "id": "yyRJQ25hXHNF"
   },
   "source": [
    "<font color='Green'><b>Answer:</b></font>\n",
    "\n",
    "- **1.1** We need to train our data to the point that it almosts overfits for unseen data, considering the fact that this model would be used to predict sensitive information regarding the possibility of a heart disease. Training the model with more than 60% missing data would more certainly than not cause bias in the model and underfitting for unseen data. The columns 'chol','fbs','slope','ca' and 'thal' all have more than 60% data missing and while we could find ways to fill in the missing values, the columns are mostly categorical. So in this case making the wrong assumptions could result in bad performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "NzUkBHBfYBzF",
   "metadata": {
    "id": "NzUkBHBfYBzF",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age          0.000000\n",
      "sex          0.000000\n",
      "cp           0.000000\n",
      "trestbps     0.340136\n",
      "chol         7.823129\n",
      "fbs          2.721088\n",
      "restecg      0.340136\n",
      "thalach      0.340136\n",
      "exang        0.340136\n",
      "oldpeak      0.000000\n",
      "slope       64.625850\n",
      "ca          98.979592\n",
      "thal        90.476190\n",
      "num          0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1.1\n",
    "# Add necessary code here.\n",
    "print(df.isna().sum() / len(df) * 100)#Print the missing value percentages here (code gotten from lab 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2607c3ea-a3d1-4781-afa0-bbf595ac3667",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age         0.000000\n",
      "sex         0.000000\n",
      "cp          0.000000\n",
      "trestbps    0.340136\n",
      "restecg     0.340136\n",
      "thalach     0.340136\n",
      "exang       0.340136\n",
      "oldpeak     0.000000\n",
      "num         0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#extract the columns with less than 60% missing values.\n",
    "\n",
    "df = df.loc[:, ['age','sex','cp','trestbps','restecg','thalach','exang','oldpeak','num']]\n",
    "\n",
    "print(df.isna().sum() / len(df) * 100)#print values of new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xkk6IDQRXgJM",
   "metadata": {
    "id": "xkk6IDQRXgJM"
   },
   "source": [
    "<font color='Green'><b>Answer:</b></font>\n",
    "\n",
    "- **1.2** ....................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "t7Hw48YkZcCb",
   "metadata": {
    "id": "t7Hw48YkZcCb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "num         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1.2\n",
    "# Add necessary code here.\n",
    "#trestbps(resting blood pressure) is continuous integer value, so we can use mean\n",
    "#restecg is categorical, use backfill or forward fill\n",
    "#thalach(maximum heart rate achieved) is also continuous integer so we can use mean\n",
    "#exang(exercise induced angina) is categorical, we'll use ffill\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "#Use SimpleImputer for thalach and trestbps\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "columns_to_impute = ['trestbps', 'thalach']\n",
    "df[columns_to_impute] = imputer.fit_transform(df[columns_to_impute])\n",
    "\n",
    "\n",
    "#Use ffill for exang and restecg\n",
    "df['restecg'] = df['restecg'].ffill()\n",
    "df['exang'] = df['exang'].bfill()\n",
    "\n",
    "#check to see if missing values are gone\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TS8GSVmmXoOg",
   "metadata": {
    "id": "TS8GSVmmXoOg"
   },
   "source": [
    "<font color='Green'><b>Answer:</b></font>\n",
    "\n",
    "- **1.3** .....................\n",
    "The columns with numerical values are age, trestbps, thalach, and oldpeak. The ones with categorical values are cp and restecg. The binary columns are sex and exang. Standard scaler is a technique for transforming numerical data to have a mean of zero and a standard deviation of one. It is useful for machine learning algorithms such as this one\n",
    "that would perform better when the input variables are scaled to a standard range. One hot encoding is a technique to convert categorical variables into\n",
    "numerical values for machine learning models.\n",
    "It creates a new column for each category and assigns a binary value of 1 or 0 simply to indicate the presence or absence of that category. This allows for improved implementation of the ML model. The 'sex' and 'exang' columns are already binary, so we 'passthrough' to indicate that no changes should be made.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pCxLaTmQXYC7",
   "metadata": {
    "id": "pCxLaTmQXYC7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num__age</th>\n",
       "      <th>num__trestbps</th>\n",
       "      <th>num__thalach</th>\n",
       "      <th>num__oldpeak</th>\n",
       "      <th>cat__cp_1</th>\n",
       "      <th>cat__cp_2</th>\n",
       "      <th>cat__cp_3</th>\n",
       "      <th>cat__cp_4</th>\n",
       "      <th>cat__restecg_0.0</th>\n",
       "      <th>cat__restecg_1.0</th>\n",
       "      <th>cat__restecg_2.0</th>\n",
       "      <th>passthrough__sex</th>\n",
       "      <th>passthrough__exang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.542347</td>\n",
       "      <td>-0.147076</td>\n",
       "      <td>1.951150</td>\n",
       "      <td>-0.646074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.414117</td>\n",
       "      <td>-0.716341</td>\n",
       "      <td>0.887744</td>\n",
       "      <td>-0.646074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.414117</td>\n",
       "      <td>0.422189</td>\n",
       "      <td>1.313106</td>\n",
       "      <td>-0.646074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.285888</td>\n",
       "      <td>2.129984</td>\n",
       "      <td>1.313106</td>\n",
       "      <td>-0.646074</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.157658</td>\n",
       "      <td>-1.854871</td>\n",
       "      <td>0.462382</td>\n",
       "      <td>-0.646074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num__age  num__trestbps  num__thalach  num__oldpeak  cat__cp_1  cat__cp_2  \\\n",
       "0 -2.542347      -0.147076      1.951150     -0.646074        0.0        1.0   \n",
       "1 -2.414117      -0.716341      0.887744     -0.646074        0.0        1.0   \n",
       "2 -2.414117       0.422189      1.313106     -0.646074        0.0        1.0   \n",
       "3 -2.285888       2.129984      1.313106     -0.646074        1.0        0.0   \n",
       "4 -2.157658      -1.854871      0.462382     -0.646074        0.0        1.0   \n",
       "\n",
       "   cat__cp_3  cat__cp_4  cat__restecg_0.0  cat__restecg_1.0  cat__restecg_2.0  \\\n",
       "0        0.0        0.0               0.0               0.0               1.0   \n",
       "1        0.0        0.0               1.0               0.0               0.0   \n",
       "2        0.0        0.0               1.0               0.0               0.0   \n",
       "3        0.0        0.0               0.0               1.0               0.0   \n",
       "4        0.0        0.0               0.0               1.0               0.0   \n",
       "\n",
       "   passthrough__sex  passthrough__exang  \n",
       "0               1.0                 0.0  \n",
       "1               1.0                 0.0  \n",
       "2               1.0                 0.0  \n",
       "3               0.0                 0.0  \n",
       "4               0.0                 0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.3\n",
    "# Add necessary code here.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "y = df.loc[:,'num']\n",
    "X = df.iloc[:,:8]\n",
    "\n",
    "# Select numerical, categorical, and binary columns\n",
    "numerical_cols = ['age', 'trestbps', 'thalach', 'oldpeak']\n",
    "categorical_cols = ['cp', 'restecg']\n",
    "binary_cols = ['sex', 'exang']\n",
    "\n",
    "# Create a ColumnTransformer with different preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),  # StandardScaler for numerical features\n",
    "        ('cat', OneHotEncoder(sparse_output = False), categorical_cols),  # OneHotEncoder for categorical features\n",
    "        ('passthrough', 'passthrough', binary_cols)  # Passthrough for binary features\n",
    "    ])\n",
    "transformed_data = preprocessor.fit_transform(X)\n",
    "transformed_df = pd.DataFrame(transformed_data, columns=preprocessor.get_feature_names_out())\n",
    "transformed_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {
    "id": "2a245d00"
   },
   "source": [
    "# **2. Pipeline and Modeling**\n",
    "\n",
    "- **2.1** Create **three** `Pipeline` objects that take the column transformer from the previous question as the first step and add one or more models as the subsequent steps. You can use any models from `sklearn` or other libraries that are suitable for binary classification. For each pipeline, explain **why** you selected the model(s) and what are their **strengths and weaknesses** for this data set. **(3 Points)**\n",
    "\n",
    "- **2.2** Use `GridSearchCV` to perform a grid search over the hyperparameters of each pipeline and find the best combination that maximizes the cross-validation score. Report the best parameters and the best score for each pipeline. Then, update the hyperparameters of each pipeline using the best parameters from the grid search. **(6 Points)**\n",
    "\n",
    "- **2.3** Form a stacking classifier that uses the three pipelines from the previous question as the base estimators and a meta-model as the `final_estimator`. You can choose any model for the meta-model that is suitable for binary classification. Explain **why** you chose the meta-model and how it combines the predictions of the base estimators. Then, use `StratifiedKFold` to perform a cross-validation on the stacking classifier and present the accuracy scores and F1 scores for each fold. Report the mean and the standard deviation of each score in the format of `mean ± std`. For example, `0.85 ± 0.05`. Interpret the results and compare them with the baseline scores from the previous assignment. **(5 Points)**\n",
    "\n",
    "- **2.4**: Interpret the final results of the stacking classifier and compare its performance with the individual models. Explain how stacking classifier has improved or deteriorated the prediction accuracy and F1 score, and what are the possible reasons for that. **(3 Points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GSpSIu-BY1Kn",
   "metadata": {
    "id": "GSpSIu-BY1Kn"
   },
   "source": [
    "<font color='Green'><b>Answer:</b></font>\n",
    "\n",
    "- **2.1** .....................\n",
    "\n",
    "**Logistic Regression:**\n",
    "\n",
    "**Strengths:**\n",
    "\n",
    "Simple and interpretable.\n",
    "\n",
    "Works well with linearly separable data.\n",
    "\n",
    "**Weaknesses:**\n",
    "\n",
    "Assumes a linear relationship between features and target.\n",
    "\n",
    "May not perform well if the decision boundary is highly non-linear.\n",
    "\n",
    "**Random Forest Classifier:**\n",
    "\n",
    "**Strengths:**\n",
    "\n",
    "Can handle both numerical and categorical features without much preprocessing.\n",
    "\n",
    "Can capture non-linear relationships between features and target.\n",
    "\n",
    "**Weaknesses:**\n",
    "\n",
    "Can be slow to train on large datasets.\n",
    "\n",
    "Less interpretable compared to simpler models like logistic regression.\n",
    "\n",
    "May not perform well with high-dimensional sparse data.\n",
    "\n",
    "**Support Vector Classifier (SVC):**\n",
    "\n",
    "**Strengths:**\n",
    "\n",
    "Effective in high-dimensional spaces, even with small datasets.\n",
    "\n",
    "Versatile as it can use different kernel functions to capture complex relationships.\n",
    "\n",
    "**Weaknesses:**\n",
    "\n",
    "Can be memory-intensive and slow to train on large datasets.\n",
    "\n",
    "Sensitive to the choice of kernel and its parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "qYMtXgFtOBMT",
   "metadata": {
    "id": "qYMtXgFtOBMT"
   },
   "outputs": [],
   "source": [
    "# 2.1\n",
    "# Add necessary code here.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Pipeline with Random Forest Classifier\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Pipeline with Logistic Regression\n",
    "pipeline_lr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Pipeline with Support Vector Classifier\n",
    "pipeline_svm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NPSo4pBVe1GR",
   "metadata": {
    "id": "NPSo4pBVe1GR"
   },
   "source": [
    "<font color='Green'><b>Answer:</b></font>\n",
    "\n",
    "- **2.2** ....................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "sNXYl9WFe3vA",
   "metadata": {
    "id": "sNXYl9WFe3vA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "Best parameters for Random Forest(Based on Accuracy): {'classifier__max_depth': 15, 'classifier__n_estimators': 100}\n",
      "Best Accuracy Score on Training Data: 0.8212765957446809\n",
      "Best Accuracy Score on Validation Data: 0.7984848484848486\n",
      "Best F1 Score on Training Data: 0.7435976488141175\n",
      "Best F1 Score on Validation Data: 0.7151515151515151\n",
      " \n",
      " \n",
      "Best parameters for Random Forest(Based on Accuracy): {'classifier__C': 1, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\n",
      "Best Accuracy Score on Training Data: 0.8382978723404255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Score on Validation Data: 0.8151515151515151\n",
      "Best F1 Score on Training Data: 0.7641324056504322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ej/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score on Validation Data: 0.7168253968253968\n",
      " \n",
      " \n",
      "Best parameters for Random Forest(Based on Accuracy): {'classifier__C': 0.1, 'classifier__kernel': 'linear'}\n",
      "Best Accuracy Score on Training Data: 0.8297872340425532\n",
      "Best Accuracy Score on Validation Data: 0.8136363636363637\n",
      "Best F1 Score on Training Data: 0.7556944230076659\n",
      "Best F1 Score on Validation Data: 0.692063492063492\n",
      " \n",
      " \n",
      "New Random Forest Classifier Train Score: 0.8978723404255319\n",
      "New Random Forest Classifier Test Score: 0.7966101694915254\n",
      "New Logistic Regression Classifier Train Score: 0.8382978723404255\n",
      "New Logistic Regression Classifier Test Score: 0.8135593220338984\n",
      "New SVM Train Score: 0.825531914893617\n",
      "New SVM Test Score: 0.8135593220338984\n"
     ]
    }
   ],
   "source": [
    "# 2.2\n",
    "# Add necessary code here.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "\n",
    "# Define parameter grids for Random Forest classifier\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [100, 200, 300],  # Step 1: Number of trees in the forest [100, 200, 300]\n",
    "    'classifier__max_depth': [5, 10, 15]          # Step 2: Maximum depth of the tree  [5, 10, 15]\n",
    "}\n",
    "\n",
    "# Define parameter grids for Logistic Regression classifier\n",
    "  # Step 1: C: Inverse of regularization strength; smaller values specify stronger regularization; Use: [0.1, 1, 10]\n",
    "  # Step 2: penalty: Type of penalty; Use: 'l1' for Lasso\n",
    "  # Step 3: solver: Algorithm to use in the optimization problem; Us: ['liblinear', 'saga']\n",
    "param_grid_lr = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__penalty': ['l1'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "\n",
    "# Define parameter grids for SVM classifier\n",
    "  # Step 1: C: Regularization parameter; Use: [0.1, 1, 10]\n",
    "  # Step 2: kernal: Kernel type to be used in the algorithm; Use: ['linear', 'rbf']\n",
    "param_grid_svm = {\n",
    "    'classifier__C' :[0.1, 1, 10],\n",
    "    'classifier__kernel' : ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create GridSearchCV instance for Random Forest algorithm\n",
    "# Parameters:\n",
    "# Random Forest pipeline, param_grid_rf, 5-fold cross-validation (cv=5), scoring metrics specified in scoring dictionary\n",
    "# Refit the model based on the F1-score to get the best model\n",
    "grid_search_rf_with_acc_score = GridSearchCV(pipeline_rf, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf_with_f1_score = GridSearchCV(pipeline_rf, param_grid_rf, cv=5, scoring='f1')\n",
    "\n",
    "# Create GridSearchCV instance for Logistic Regression algorithm\n",
    "# Step 1: Perform grid search using Logistic Regression pipeline\n",
    "# Step 2: Use parameter combinations defined in param_grid_lr\n",
    "# Step 3: Use 5-fold cross-validation (cv=5) for model evaluation\n",
    "# Step 4: Use scoring metrics specified ealier and  n_jobs=-1\n",
    "# Step 5: Refit the model based on the F1-score to get the best model\n",
    "grid_search_lr_with_acc_score = GridSearchCV(pipeline_lr, param_grid_lr, cv=5, scoring='accuracy' )\n",
    "grid_search_lr_with_f1_score = GridSearchCV(pipeline_lr, param_grid_lr, cv=5, scoring='f1' )\n",
    "\n",
    "\n",
    "# Create GridSearchCV instance for SVM algorithm\n",
    "# Step 1: Perform grid search using SVM pipeline\n",
    "# Step 2: Use parameter combinations defined in param_grid_svm\n",
    "# Step 3: Use 5-fold cross-validation (cv=5) for model evaluation\n",
    "# Step 4: Use scoring metrics specified ealier and  n_jobs=-1\n",
    "# Step 5: Refit the model based on the F1-score to get the best model\n",
    "grid_search_svm_with_acc_score = GridSearchCV(pipeline_svm, param_grid_svm, cv=5, scoring='accuracy')\n",
    "grid_search_svm_with_f1_score = GridSearchCV(pipeline_svm, param_grid_svm, cv=5, scoring='f1')\n",
    "\n",
    "\n",
    "#Split X and y into a train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "#Fit the grid search models for the training data\n",
    "\n",
    "\n",
    "# Fit the Random Forest model\n",
    "grid_search_rf_with_acc_score.fit(X_train, y_train)\n",
    "grid_search_rf_with_f1_score.fit(X_train, y_train)\n",
    "\n",
    "# Fit the Logistic Regression model\n",
    "grid_search_lr_with_acc_score.fit(X_train, y_train)\n",
    "grid_search_lr_with_f1_score.fit(X_train, y_train)\n",
    "\n",
    "# Fit the SVM model\n",
    "grid_search_svm_with_acc_score.fit(X_train, y_train)\n",
    "grid_search_svm_with_f1_score.fit(X_train, y_train)\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "\n",
    "# Get the best parameters for Random Forest based on F1 score\n",
    "best_params_rf_with_acc = grid_search_rf_with_acc_score.best_params_\n",
    "best_params_rf_with_f1 = grid_search_rf_with_f1_score.best_params_\n",
    "\n",
    "# Get the best parameters for Logistic Regression based on F1 score\n",
    "best_params_lr_with_acc = grid_search_lr_with_acc_score.best_params_\n",
    "best_params_lr_with_f1 = grid_search_lr_with_f1_score.best_params_\n",
    "\n",
    "\n",
    "# Get the best parameters for SVM based on F1 score\n",
    "best_params_svm_with_acc = grid_search_svm_with_acc_score.best_params_\n",
    "best_params_svm_with_f1 = grid_search_svm_with_f1_score.best_params_\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('Best parameters for Random Forest(Based on Accuracy):', best_params_rf_with_acc)\n",
    "print(\"Best Accuracy Score on Training Data:\", grid_search_rf_with_acc_score.best_score_)\n",
    "grid_search_rf_with_acc_score.fit(X_test, y_test)#Now we fit for validation data\n",
    "print(\"Best Accuracy Score on Validation Data:\", grid_search_rf_with_acc_score.best_score_)\n",
    "print(\"Best F1 Score on Training Data:\", grid_search_rf_with_f1_score.best_score_)\n",
    "grid_search_rf_with_f1_score.fit(X_test, y_test)#Now we fit for validation data\n",
    "print(\"Best F1 Score on Validation Data:\", grid_search_rf_with_f1_score.best_score_)\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('Best parameters for Logistic Regression(Based on Accuracy):', best_params_lr_with_acc)\n",
    "print(\"Best Accuracy Score on Training Data:\", grid_search_lr_with_acc_score.best_score_)\n",
    "grid_search_lr_with_acc_score.fit(X_test, y_test)#Now we fit for validation data\n",
    "print(\"Best Accuracy Score on Validation Data:\", grid_search_lr_with_acc_score.best_score_)\n",
    "print(\"Best F1 Score on Training Data:\", grid_search_lr_with_f1_score.best_score_)\n",
    "grid_search_lr_with_f1_score.fit(X_test, y_test)#Now we fit for validation data\n",
    "print(\"Best F1 Score on Validation Data:\", grid_search_lr_with_f1_score.best_score_)\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('Best parameters for SVM(Based on Accuracy):', best_params_svm_with_acc)\n",
    "print(\"Best Accuracy Score on Training Data:\", grid_search_svm_with_acc_score.best_score_)\n",
    "grid_search_svm_with_acc_score.fit(X_test, y_test)#Now we fit for validation data\n",
    "print(\"Best Accuracy Score on Validation Data:\", grid_search_svm_with_acc_score.best_score_)\n",
    "print(\"Best F1 Score on Training Data:\", grid_search_svm_with_f1_score.best_score_)\n",
    "grid_search_svm_with_f1_score.fit(X_test, y_test)#Now we fit for validation data\n",
    "print(\"Best F1 Score on Validation Data:\", grid_search_svm_with_f1_score.best_score_)\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "# Rebuild a Random Forest model using the best parameters obtained from the parameter search\n",
    "#Note best parameters are based on best Accuracy from previous step\n",
    "\n",
    "rfc = RandomForestClassifier(random_state = 0, max_depth=5, n_estimators=300)\n",
    "lr = LogisticRegression(random_state = 0, C=1, penalty='l1',solver='liblinear')\n",
    "svm = SVC(C=0.1, kernel='linear')\n",
    "\n",
    "# Train the model\n",
    "rfc.fit(X_train, y_train)\n",
    "lr.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "# Evaluate the model on the training and test sets\n",
    "rfc_train_score = rfc.score(X_train, y_train)\n",
    "rfc_test_score = rfc.score(X_test, y_test)\n",
    "\n",
    "lr_train_score = lr.score(X_train, y_train)\n",
    "lr_test_score = lr.score(X_test, y_test)\n",
    "\n",
    "svm_train_score = svm.score(X_train, y_train)\n",
    "svm_test_score = svm.score(X_test, y_test)\n",
    "\n",
    "print('New Random Forest Classifier Train Score:', rfc_train_score)\n",
    "print('New Random Forest Classifier Test Score:', rfc_test_score)\n",
    "\n",
    "print('New Logistic Regression Classifier Train Score:', lr_train_score)\n",
    "print('New Logistic Regression Classifier Test Score:', lr_test_score)\n",
    "\n",
    "print('New SVM Train Score:', svm_train_score)\n",
    "print('New SVM Test Score:', svm_test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ygOeNB-PamnU",
   "metadata": {
    "id": "ygOeNB-PamnU"
   },
   "source": [
    "<font color='Green'><b>Answer:</b></font>\n",
    "\n",
    "- **2.3** .....................\n",
    " Logistic Regression is chosen as the meta-model because it is simple, interpretable, and performs well in many cases of binary classification. It is also less prone to overfitting and works well with stacked models. StackingClassifier combines the predictions of the base estimators by training a meta-model on their outputs. In this case, the meta-model (Logistic Regression) takes the predictions of the base estimators as input features and learns to make the final prediction based on them. StratifiedKFold is used for cross-validation to ensure that each fold has approximately the same proportion of positive and negative samples as the original dataset. This helps in obtaining reliable estimates of the model performance.\n",
    " \n",
    " In the last, assignment, we used the SVC and DecisionTreeClassifier models to make classification predictions on the wine dataset. We found that we got 70 and 66 percent Training and Testing accuracy(respectively) with the SVC model. We got 97 and 87 percent Training and Testing accuracy(respectively) with the Decision Tree Classifier. From the results of our StackingClassifier, we can see that it does better than the SVC model but slightly worse than the Decision Tree Classifier in terms of Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "UvhsbjmYP2G_",
   "metadata": {
    "id": "UvhsbjmYP2G_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81 ± 0.05\n",
      "F1 Score: 0.71 ± 0.10\n"
     ]
    }
   ],
   "source": [
    "# 2.3\n",
    "# Add necessary code here.\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "# Define base estimators\n",
    "base_estimators = [\n",
    "    ('rf', pipeline_rf),\n",
    "    ('lr', pipeline_lr),\n",
    "    ('svm', pipeline_svm)\n",
    "]\n",
    "\n",
    "# Define meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Create StackingClassifier\n",
    "stacking_classifier = StackingClassifier(estimators=base_estimators, final_estimator=meta_model)\n",
    "\n",
    "# Perform cross-validation with StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation then get accuracy and F1 scores for each fold\n",
    "accuracy_scores = cross_val_score(stacking_classifier, X, y, cv=cv, scoring='accuracy')\n",
    "f1_scores = cross_val_score(stacking_classifier, X, y, cv=cv, scoring='f1')\n",
    "\n",
    "# Report mean and standard deviation of accuracy and F1 scores\n",
    "mean_accuracy = accuracy_scores.mean()\n",
    "std_accuracy = accuracy_scores.std()\n",
    "mean_f1 = f1_scores.mean()\n",
    "std_f1 = f1_scores.std()\n",
    "\n",
    "print(\"Accuracy: {:.2f} ± {:.2f}\".format(mean_accuracy, std_accuracy))\n",
    "print(\"F1 Score: {:.2f} ± {:.2f}\".format(mean_f1, std_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A-TN9hr3b77-",
   "metadata": {
    "id": "A-TN9hr3b77-"
   },
   "source": [
    "<font color='Green'><b>Answer:</b></font>\n",
    "\n",
    "- **2.4** .....................\n",
    "The accuracy of StackingClassifier is 0.81 ± 0.05. The F1-Score is 0.71 ± 0.10. The training accuracy and f1 of the RandomForestClassifier is 0.8212765957446809 and 0.7435976488141 respectively. The training accuracy and f1 of the LogisticRegressionModel is 0.8382978723404255 and 0.7641324056504 respectively. The training accuracy and f1 of the SVC Model is 0.8297872340425532 and 0.755694423007 respectively. The accuracy and f1-score of the stacking classifier is in line with the training accuracies of the individual models. This suggests that the stacking classifier has not significantly improved or deteriorated the prediction accuracy compared to the individual models. One reason for this might be that if the individual models have similar biases or make similar errors, the stacking classifier may not provide significant improvements in accuracy. Additionally, if the base models are highly correlated or have similar weaknesses, the stacking classifier's performance may not surpass that of the individual models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RPa-v8Xxc7aU",
   "metadata": {
    "id": "RPa-v8Xxc7aU"
   },
   "source": [
    "**Bonus Question**: The stacking classifier has achieved a high accuracy and F1 score, but there may be still room for improvement. Suggest **two** possible ways to improve the modeling using the stacking classifier, and explain **how** and **why** they could improve the performance. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IrSooo0DfC-V",
   "metadata": {
    "id": "IrSooo0DfC-V"
   },
   "source": [
    "<font color='Green'><b>Answer:</b></font>\n",
    "\n",
    "Model Diversity:\n",
    "\n",
    " Increase the diversity of the base models used in the stacking classifier. Including models with different algorithms, such as decision trees, k-nearest neighbors, or gradient boosting machines, can capture different aspects of the data and reduce the likelihood of correlated errors. Diversity in base models helps the stacking classifier generalize better to unseen data.\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "\n",
    " Explanation: Optimize the hyperparameters of both the base models and the meta-model in the stacking classifier. Fine-tuning hyperparameters can significantly impact model performance by finding the optimal configuration for each model and the ensemble as a whole. By tuning hyperparameters, you can potentially improve the generalization ability of the stacking classifier and achieve better performance on unseen data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
